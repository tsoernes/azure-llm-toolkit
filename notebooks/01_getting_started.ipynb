{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Azure LLM Toolkit\n",
    "\n",
    "This notebook provides a comprehensive introduction to the Azure LLM Toolkit, covering:\n",
    "\n",
    "1. Installation and setup\n",
    "2. Basic configuration\n",
    "3. Simple chat completion\n",
    "4. Streaming responses\n",
    "5. Function calling / tools\n",
    "6. Cost tracking\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure OpenAI API access\n",
    "- Python 3.9+\n",
    "- API key and endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, install the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (run this once)\n",
    "# !pip install azure-llm-toolkit\n",
    "\n",
    "# Or for development:\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure your Azure OpenAI credentials. There are multiple ways to do this:\n",
    "\n",
    "### Option A: Environment Variables (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource.openai.azure.com\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"gpt-4\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-15-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Configuration Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit import AzureConfig\n",
    "\n",
    "config = AzureConfig(\n",
    "    api_key=\"your-api-key-here\",\n",
    "    endpoint=\"https://your-resource.openai.azure.com\",\n",
    "    deployment=\"gpt-4\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "print(f\"Configuration loaded for deployment: {config.deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Client\n",
    "\n",
    "Create an instance of the `AzureLLMClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit import AzureLLMClient\n",
    "\n",
    "# Using environment variables\n",
    "client = AzureLLMClient()\n",
    "\n",
    "# Or using config object\n",
    "# client = AzureLLMClient(config=config)\n",
    "\n",
    "print(\"‚úÖ Client created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Chat Completion\n",
    "\n",
    "Let's send a simple chat completion request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple synchronous request\n",
    "response = await client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Usage Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nUsage Statistics:\")\n",
    "print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"  Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses\n",
    "\n",
    "Stream responses in real-time for a better user experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response: \", end=\"\", flush=True)\n",
    "\n",
    "async for chunk in client.chat_completion_stream(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about coding.\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "):\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the Sync Client\n",
    "\n",
    "If you're working in a non-async environment, use the sync client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit import SyncAzureLLMClient\n",
    "\n",
    "sync_client = SyncAzureLLMClient()\n",
    "\n",
    "# No async/await needed\n",
    "response = sync_client.chat_completion(messages=[{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}], max_tokens=50)\n",
    "\n",
    "print(f\"Answer: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Function Calling / Tools\n",
    "\n",
    "Use function calling to enable the LLM to interact with external tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit.tools import tool, ToolRegistry\n",
    "\n",
    "# Create a tool registry\n",
    "registry = ToolRegistry()\n",
    "\n",
    "\n",
    "# Define a tool using the decorator\n",
    "@tool(registry=registry)\n",
    "def get_weather(location: str, unit: str = \"celsius\") -> dict:\n",
    "    \"\"\"Get the current weather for a location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. 'San Francisco, CA'\n",
    "        unit: Temperature unit, either 'celsius' or 'fahrenheit'\n",
    "\n",
    "    Returns:\n",
    "        Weather information dictionary\n",
    "    \"\"\"\n",
    "    # Mock weather data\n",
    "    return {\"location\": location, \"temperature\": 22, \"unit\": unit, \"condition\": \"sunny\"}\n",
    "\n",
    "\n",
    "@tool(registry=registry)\n",
    "def calculate(expression: str) -> float:\n",
    "    \"\"\"Evaluate a mathematical expression.\n",
    "\n",
    "    Args:\n",
    "        expression: A math expression like '2 + 2' or '10 * 5'\n",
    "\n",
    "    Returns:\n",
    "        The result of the calculation\n",
    "    \"\"\"\n",
    "    return eval(expression)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Registered {len(registry.list_tools())} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tools with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request with tools\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Paris and what is 15 times 7?\"}]\n",
    "\n",
    "response = await client.chat_completion(messages=messages, tools=registry.to_azure_tools(), tool_choice=\"auto\")\n",
    "\n",
    "# Check if the model wants to call tools\n",
    "if response.choices[0].message.tool_calls:\n",
    "    print(\"üîß Model wants to call tools:\")\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        print(f\"  - {tool_call.function.name}({tool_call.function.arguments})\")\n",
    "\n",
    "    # Execute the tool calls\n",
    "    tool_results = registry.execute_tool_calls(response.choices[0].message.tool_calls)\n",
    "\n",
    "    # Add assistant message with tool calls\n",
    "    messages.append(response.choices[0].message)\n",
    "\n",
    "    # Add tool results\n",
    "    for result in tool_results:\n",
    "        messages.append({\"role\": \"tool\", \"tool_call_id\": result.tool_call_id, \"content\": result.content})\n",
    "\n",
    "    # Get final response\n",
    "    final_response = await client.chat_completion(messages=messages, tools=registry.to_azure_tools())\n",
    "\n",
    "    print(\"\\nüìù Final Response:\")\n",
    "    print(final_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"üí¨ Direct response:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Tracking\n",
    "\n",
    "Monitor costs automatically with the built-in cost tracker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cost statistics\n",
    "cost_stats = client.get_cost_stats()\n",
    "\n",
    "print(\"üí∞ Cost Statistics:\")\n",
    "print(f\"  Total requests: {cost_stats['total_requests']}\")\n",
    "print(f\"  Total tokens: {cost_stats['total_tokens']:,}\")\n",
    "print(f\"  Prompt tokens: {cost_stats['prompt_tokens']:,}\")\n",
    "print(f\"  Completion tokens: {cost_stats['completion_tokens']:,}\")\n",
    "print(f\"  Total cost: ${cost_stats['total_cost']:.4f}\")\n",
    "print(f\"  Average cost per request: ${cost_stats['avg_cost_per_request']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Embeddings\n",
    "\n",
    "Generate embeddings for text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single text embedding\n",
    "embedding = await client.embed_text(\n",
    "    text=\"Azure LLM Toolkit makes it easy to work with Azure OpenAI\",\n",
    "    deployment=\"text-embedding-ada-002\",  # Use your embedding deployment\n",
    ")\n",
    "\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch embeddings\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Python is a great programming language\",\n",
    "]\n",
    "\n",
    "embeddings = await client.embed_texts(texts=texts, deployment=\"text-embedding-ada-002\")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Caching\n",
    "\n",
    "Enable caching to avoid redundant API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit import AzureLLMClient\n",
    "from azure_llm_toolkit.cache import InMemoryCache\n",
    "\n",
    "# Create client with caching enabled\n",
    "cached_client = AzureLLMClient(\n",
    "    cache=InMemoryCache(ttl_seconds=3600)  # Cache for 1 hour\n",
    ")\n",
    "\n",
    "# First call - hits the API\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "response1 = await cached_client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 42 + 58?\"}], max_tokens=50\n",
    ")\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Second call - from cache\n",
    "start = time.time()\n",
    "response2 = await cached_client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 42 + 58?\"}], max_tokens=50\n",
    ")\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"First call: {time1:.3f}s\")\n",
    "print(f\"Cached call: {time2:.3f}s\")\n",
    "print(f\"Speedup: {time1 / time2:.1f}x faster!\")\n",
    "print(f\"\\nResponse: {response2.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Health Checks\n",
    "\n",
    "Check the health of your Azure OpenAI connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit.health import health_check\n",
    "\n",
    "# Perform health check\n",
    "health = await health_check(client)\n",
    "\n",
    "print(f\"Health Status: {'‚úÖ Healthy' if health['healthy'] else '‚ùå Unhealthy'}\")\n",
    "print(f\"Timestamp: {health['timestamp']}\")\n",
    "\n",
    "if health[\"checks\"]:\n",
    "    print(\"\\nDetailed Checks:\")\n",
    "    for check_name, check_result in health[\"checks\"].items():\n",
    "        status = \"‚úÖ\" if check_result.get(\"healthy\", False) else \"‚ùå\"\n",
    "        print(f\"  {status} {check_name}: {check_result.get('message', 'OK')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Clean Up\n",
    "\n",
    "Always close the client when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.close()\n",
    "await cached_client.close()\n",
    "\n",
    "print(\"‚úÖ Clients closed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've learned the basics, check out these other notebooks:\n",
    "\n",
    "1. **02_rate_limiting_strategies.ipynb** - Learn how to handle rate limits effectively\n",
    "2. **03_cost_optimization.ipynb** - Techniques for reducing costs\n",
    "3. **04_rag_implementation.ipynb** - Build a RAG system\n",
    "4. **05_agent_patterns.ipynb** - Create intelligent agents\n",
    "5. **06_production_deployment.ipynb** - Deploy to production\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/tsoernes/azure-llm-toolkit)\n",
    "- [Full Documentation](https://github.com/tsoernes/azure-llm-toolkit/blob/main/README.md)\n",
    "- [Examples](https://github.com/tsoernes/azure-llm-toolkit/tree/main/examples)\n",
    "- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
