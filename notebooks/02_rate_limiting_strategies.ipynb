{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate Limiting Strategies with Azure LLM Toolkit\n",
    "\n",
    "This notebook demonstrates advanced rate limiting strategies to handle Azure OpenAI API limits effectively.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. Understanding Azure OpenAI rate limits\n",
    "2. Token bucket algorithm\n",
    "3. Sliding window rate limiting\n",
    "4. Adaptive rate limiting\n",
    "5. Concurrent request management\n",
    "6. Handling 429 errors gracefully\n",
    "7. Best practices for high-throughput applications\n",
    "\n",
    "## Azure OpenAI Rate Limits\n",
    "\n",
    "Azure OpenAI enforces several types of rate limits:\n",
    "- **TPM (Tokens Per Minute)**: Total tokens processed per minute\n",
    "- **RPM (Requests Per Minute)**: Total API requests per minute\n",
    "- **Concurrent Requests**: Maximum simultaneous requests\n",
    "\n",
    "Exceeding these limits results in HTTP 429 (Too Many Requests) errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from azure_llm_toolkit import AzureLLMClient, AzureConfig\n",
    "from azure_llm_toolkit.rate_limiter import RateLimiter, RateLimitConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Rate Limiting\n",
    "\n",
    "The toolkit includes automatic rate limiting with sensible defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client with default rate limiting\n",
    "client = AzureLLMClient()\n",
    "\n",
    "# The client automatically handles rate limits\n",
    "print(\"‚úÖ Client created with automatic rate limiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Basic Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_basic_rate_limiting():\n",
    "    \"\"\"Send multiple requests and observe rate limiting in action.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Say 'hello' in one word.\"}]\n",
    "\n",
    "    print(\"Sending 10 requests...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(10):\n",
    "        response = await client.chat_completion(messages=messages, max_tokens=10)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Request {i + 1}: {response.choices[0].message.content.strip()[:20]} (elapsed: {elapsed:.2f}s)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Completed 10 requests in {total_time:.2f}s\")\n",
    "    print(f\"Average: {total_time / 10:.2f}s per request\")\n",
    "\n",
    "\n",
    "await test_basic_rate_limiting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Rate Limit Configuration\n",
    "\n",
    "Configure rate limits to match your Azure deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure rate limits based on your Azure deployment\n",
    "rate_limit_config = RateLimitConfig(\n",
    "    max_requests_per_minute=60,  # RPM limit\n",
    "    max_tokens_per_minute=90000,  # TPM limit\n",
    "    max_concurrent_requests=10,  # Concurrent request limit\n",
    "    retry_max_attempts=5,  # Number of retry attempts\n",
    "    retry_initial_delay=1.0,  # Initial retry delay (seconds)\n",
    "    retry_max_delay=60.0,  # Maximum retry delay (seconds)\n",
    "    retry_exponential_base=2.0,  # Exponential backoff multiplier\n",
    ")\n",
    "\n",
    "# Create client with custom configuration\n",
    "custom_client = AzureLLMClient(rate_limit_config=rate_limit_config)\n",
    "\n",
    "print(\"‚úÖ Client created with custom rate limits:\")\n",
    "print(f\"  RPM: {rate_limit_config.max_requests_per_minute}\")\n",
    "print(f\"  TPM: {rate_limit_config.max_tokens_per_minute}\")\n",
    "print(f\"  Concurrent: {rate_limit_config.max_concurrent_requests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token Bucket Algorithm\n",
    "\n",
    "The toolkit uses a token bucket algorithm for smooth rate limiting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standalone rate limiter to observe its behavior\n",
    "rate_limiter = RateLimiter(max_requests_per_minute=30, max_tokens_per_minute=10000)\n",
    "\n",
    "\n",
    "async def demonstrate_token_bucket():\n",
    "    \"\"\"Demonstrate token bucket behavior.\"\"\"\n",
    "\n",
    "    print(\"Token Bucket Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Try to acquire tokens\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "\n",
    "        # Acquire 100 tokens\n",
    "        await rate_limiter.acquire(tokens=100)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        stats = rate_limiter.get_stats()\n",
    "\n",
    "        print(f\"\\nAcquisition {i + 1}:\")\n",
    "        print(f\"  Wait time: {elapsed:.3f}s\")\n",
    "        print(f\"  Tokens available: {stats['tokens_available']:.0f}\")\n",
    "        print(f\"  Requests available: {stats['requests_available']:.0f}\")\n",
    "\n",
    "\n",
    "await demonstrate_token_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Concurrent Requests\n",
    "\n",
    "Process multiple requests concurrently while respecting rate limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_with_concurrency(prompts: List[str], max_concurrent: int = 5):\n",
    "    \"\"\"Process a batch of prompts with controlled concurrency.\"\"\"\n",
    "\n",
    "    # Configure client with concurrency limit\n",
    "    batch_client = AzureLLMClient(rate_limit_config=RateLimitConfig(max_concurrent_requests=max_concurrent))\n",
    "\n",
    "    async def process_one(prompt: str, idx: int):\n",
    "        \"\"\"Process a single prompt.\"\"\"\n",
    "        start = time.time()\n",
    "        response = await batch_client.chat_completion(messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=50)\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            \"idx\": idx,\n",
    "            \"prompt\": prompt[:30] + \"...\",\n",
    "            \"response\": response.choices[0].message.content[:50],\n",
    "            \"time\": elapsed,\n",
    "        }\n",
    "\n",
    "    # Process all prompts concurrently\n",
    "    start_time = time.time()\n",
    "    tasks = [process_one(prompt, i) for i, prompt in enumerate(prompts)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    await batch_client.close()\n",
    "\n",
    "    return results, total_time\n",
    "\n",
    "\n",
    "# Test with 20 prompts\n",
    "test_prompts = [f\"What is {i} + {i + 1}?\" for i in range(20)]\n",
    "\n",
    "results, total_time = await process_batch_with_concurrency(test_prompts, max_concurrent=5)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(results)} prompts in {total_time:.2f}s\")\n",
    "print(f\"Average time per request: {total_time / len(results):.2f}s\")\n",
    "print(f\"Throughput: {len(results) / total_time:.2f} requests/second\")\n",
    "print(f\"\\nFirst 3 results:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"  [{result['idx']}] {result['prompt']} -> {result['response'][:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adaptive Rate Limiting\n",
    "\n",
    "Automatically adjust rate limits based on API responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit.rate_limiter import AdaptiveRateLimiter\n",
    "\n",
    "# Create adaptive rate limiter\n",
    "adaptive_limiter = AdaptiveRateLimiter(\n",
    "    initial_rpm=60,\n",
    "    initial_tpm=90000,\n",
    "    adjustment_factor=0.8,  # Reduce by 20% on 429 errors\n",
    "    recovery_factor=1.1,  # Increase by 10% on success\n",
    ")\n",
    "\n",
    "adaptive_client = AzureLLMClient(rate_limiter=adaptive_limiter)\n",
    "\n",
    "print(\"‚úÖ Client created with adaptive rate limiting\")\n",
    "print(f\"Initial RPM: {adaptive_limiter.current_rpm}\")\n",
    "print(f\"Initial TPM: {adaptive_limiter.current_tpm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Adaptive Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_adaptive_limiting():\n",
    "    \"\"\"Test adaptive rate limiting behavior.\"\"\"\n",
    "\n",
    "    print(\"Testing adaptive rate limiting...\\n\")\n",
    "\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            response = await adaptive_client.chat_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Count to {i + 1}\"}], max_tokens=20\n",
    "            )\n",
    "\n",
    "            stats = adaptive_limiter.get_stats()\n",
    "            print(f\"Request {i + 1}: Success\")\n",
    "            print(f\"  Current RPM: {stats['current_rpm']:.0f}\")\n",
    "            print(f\"  Current TPM: {stats['current_tpm']:.0f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i + 1}: Error - {str(e)}\")\n",
    "            stats = adaptive_limiter.get_stats()\n",
    "            print(f\"  Adjusted RPM: {stats['current_rpm']:.0f}\")\n",
    "            print(f\"  Adjusted TPM: {stats['current_tpm']:.0f}\")\n",
    "\n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "\n",
    "await test_adaptive_limiting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling 429 Errors\n",
    "\n",
    "Best practices for handling rate limit errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit.exceptions import RateLimitError\n",
    "\n",
    "\n",
    "async def safe_request_with_retry(messages: List[dict], max_retries: int = 5):\n",
    "    \"\"\"Make a request with custom retry logic.\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = await client.chat_completion(messages=messages, max_tokens=100)\n",
    "            return response\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2**attempt  # Exponential backoff\n",
    "                print(f\"‚ö†Ô∏è  Rate limit hit (attempt {attempt + 1}/{max_retries})\")\n",
    "                print(f\"   Waiting {wait_time}s before retry...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"‚ùå Failed after {max_retries} attempts\")\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Test the retry logic\n",
    "try:\n",
    "    response = await safe_request_with_retry(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "    print(f\"‚úÖ Success: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Final error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitoring Rate Limit Usage\n",
    "\n",
    "Track rate limit utilization in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rate_limit_stats(client: AzureLLMClient):\n",
    "    \"\"\"Print current rate limit statistics.\"\"\"\n",
    "\n",
    "    if hasattr(client, \"rate_limiter\"):\n",
    "        stats = client.rate_limiter.get_stats()\n",
    "\n",
    "        print(\"üìä Rate Limit Statistics\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Request limits\n",
    "        rpm_used = stats[\"max_requests_per_minute\"] - stats[\"requests_available\"]\n",
    "        rpm_percent = (rpm_used / stats[\"max_requests_per_minute\"]) * 100\n",
    "        print(f\"Requests:\")\n",
    "        print(f\"  Used: {rpm_used:.0f}/{stats['max_requests_per_minute']:.0f} ({rpm_percent:.1f}%)\")\n",
    "        print(f\"  Available: {stats['requests_available']:.0f}\")\n",
    "\n",
    "        # Token limits\n",
    "        tpm_used = stats[\"max_tokens_per_minute\"] - stats[\"tokens_available\"]\n",
    "        tpm_percent = (tpm_used / stats[\"max_tokens_per_minute\"]) * 100\n",
    "        print(f\"\\nTokens:\")\n",
    "        print(f\"  Used: {tpm_used:.0f}/{stats['max_tokens_per_minute']:.0f} ({tpm_percent:.1f}%)\")\n",
    "        print(f\"  Available: {stats['tokens_available']:.0f}\")\n",
    "\n",
    "        # Concurrent requests\n",
    "        print(f\"\\nConcurrent Requests: {stats.get('active_requests', 0)}/{stats.get('max_concurrent', 'N/A')}\")\n",
    "\n",
    "        # Errors\n",
    "        print(f\"\\nErrors:\")\n",
    "        print(f\"  Rate limit errors: {stats.get('rate_limit_errors', 0)}\")\n",
    "        print(f\"  Retry attempts: {stats.get('retry_attempts', 0)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Rate limiter not available\")\n",
    "\n",
    "\n",
    "# Print current stats\n",
    "print_rate_limit_stats(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing with Rate Limiting\n",
    "\n",
    "Use the batch runner for efficient processing of many requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_llm_toolkit.batch import ChatBatchRunner, ChatBatchItem\n",
    "\n",
    "\n",
    "async def process_large_batch():\n",
    "    \"\"\"Process a large batch of requests efficiently.\"\"\"\n",
    "\n",
    "    # Create batch items\n",
    "    items = [\n",
    "        ChatBatchItem(\n",
    "            id=f\"question_{i}\", messages=[{\"role\": \"user\", \"content\": f\"What is {i} squared?\"}], max_tokens=20\n",
    "        )\n",
    "        for i in range(50)\n",
    "    ]\n",
    "\n",
    "    # Create batch runner with rate limiting\n",
    "    runner = ChatBatchRunner(\n",
    "        client=client,\n",
    "        max_concurrent=10,  # Process 10 at a time\n",
    "        show_progress=True,  # Show progress bar\n",
    "    )\n",
    "\n",
    "    # Process batch\n",
    "    print(\"Processing 50 chat completions...\\n\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = await runner.run(items)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Analyze results\n",
    "    successful = sum(1 for r in results if r.success)\n",
    "    failed = len(results) - successful\n",
    "\n",
    "    print(f\"\\n‚úÖ Batch processing complete!\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Throughput: {successful / total_time:.2f} requests/second\")\n",
    "\n",
    "    # Show sample results\n",
    "    print(f\"\\nSample results:\")\n",
    "    for result in results[:3]:\n",
    "        if result.success:\n",
    "            content = result.response.choices[0].message.content\n",
    "            print(f\"  {result.id}: {content[:50]}...\")\n",
    "\n",
    "\n",
    "await process_large_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices Summary\n",
    "\n",
    "### Do's ‚úÖ\n",
    "\n",
    "1. **Configure limits based on your deployment**: Check your Azure portal for actual limits\n",
    "2. **Use batch processing**: For many requests, use `ChatBatchRunner` or `EmbeddingBatchRunner`\n",
    "3. **Implement exponential backoff**: On 429 errors, wait progressively longer\n",
    "4. **Monitor usage**: Track rate limit utilization to optimize throughput\n",
    "5. **Use adaptive limiting**: Let the system adjust automatically based on API responses\n",
    "6. **Set appropriate concurrency**: More isn't always better; match your deployment capacity\n",
    "\n",
    "### Don'ts ‚ùå\n",
    "\n",
    "1. **Don't ignore rate limits**: Always configure them; default values may not match your deployment\n",
    "2. **Don't retry immediately**: Always wait before retrying after a 429 error\n",
    "3. **Don't set excessive concurrency**: This wastes resources and increases latency\n",
    "4. **Don't forget to close clients**: Always call `await client.close()` when done\n",
    "5. **Don't use synchronous code for high throughput**: Use async for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Dynamic Rate Limit Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRateLimitManager:\n",
    "    \"\"\"Dynamically adjust rate limits based on system load.\"\"\"\n",
    "\n",
    "    def __init__(self, client: AzureLLMClient):\n",
    "        self.client = client\n",
    "        self.error_count = 0\n",
    "        self.success_count = 0\n",
    "        self.adjustment_threshold = 5\n",
    "\n",
    "    async def adaptive_request(self, messages: List[dict], **kwargs):\n",
    "        \"\"\"Make a request with adaptive rate limiting.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await self.client.chat_completion(messages=messages, **kwargs)\n",
    "            self.success_count += 1\n",
    "\n",
    "            # Gradually increase limits on success\n",
    "            if self.success_count >= self.adjustment_threshold:\n",
    "                self._increase_limits()\n",
    "                self.success_count = 0\n",
    "\n",
    "            return response\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            self.error_count += 1\n",
    "\n",
    "            # Decrease limits on errors\n",
    "            if self.error_count >= 2:\n",
    "                self._decrease_limits()\n",
    "                self.error_count = 0\n",
    "\n",
    "            raise\n",
    "\n",
    "    def _increase_limits(self):\n",
    "        \"\"\"Increase rate limits by 5%.\"\"\"\n",
    "        if hasattr(self.client, \"rate_limiter\"):\n",
    "            limiter = self.client.rate_limiter\n",
    "            limiter.max_requests_per_minute *= 1.05\n",
    "            limiter.max_tokens_per_minute *= 1.05\n",
    "            print(\n",
    "                f\"üìà Increased limits: RPM={limiter.max_requests_per_minute:.0f}, TPM={limiter.max_tokens_per_minute:.0f}\"\n",
    "            )\n",
    "\n",
    "    def _decrease_limits(self):\n",
    "        \"\"\"Decrease rate limits by 20%.\"\"\"\n",
    "        if hasattr(self.client, \"rate_limiter\"):\n",
    "            limiter = self.client.rate_limiter\n",
    "            limiter.max_requests_per_minute *= 0.8\n",
    "            limiter.max_tokens_per_minute *= 0.8\n",
    "            print(\n",
    "                f\"üìâ Decreased limits: RPM={limiter.max_requests_per_minute:.0f}, TPM={limiter.max_tokens_per_minute:.0f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Test dynamic manager\n",
    "manager = DynamicRateLimitManager(client)\n",
    "\n",
    "print(\"Testing dynamic rate limit adjustment...\\n\")\n",
    "for i in range(10):\n",
    "    try:\n",
    "        response = await manager.adaptive_request(messages=[{\"role\": \"user\", \"content\": f\"Number {i}\"}], max_tokens=10)\n",
    "        print(f\"‚úÖ Request {i + 1} succeeded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Request {i + 1} failed: {str(e)[:50]}\")\n",
    "\n",
    "    await asyncio.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all clients\n",
    "await client.close()\n",
    "await custom_client.close()\n",
    "await adaptive_client.close()\n",
    "\n",
    "print(\"‚úÖ All clients closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **03_cost_optimization.ipynb**: Learn how to minimize API costs\n",
    "- **04_rag_implementation.ipynb**: Build a RAG system\n",
    "- **05_agent_patterns.ipynb**: Create intelligent agents\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Azure OpenAI Rate Limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)\n",
    "- [Token Bucket Algorithm](https://en.wikipedia.org/wiki/Token_bucket)\n",
    "- [Examples Directory](https://github.com/tsoernes/azure-llm-toolkit/tree/main/examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
