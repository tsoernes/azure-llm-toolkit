# Azure LLM Toolkit - Live Prometheus Metrics Summary

**Date**: December 10, 2025  
**Status**: âœ… ALL SYSTEMS OPERATIONAL

---

## ğŸ”´ LIVE METRICS - Real Azure OpenAI Data

All dashboards are showing **REAL data** from actual Azure OpenAI API calls.

### ğŸ“Š Current Metrics (Real-Time)

**Requests Per Minute (RPM)**:
- ğŸ’¬ Chat Completion: ~4.36 requests/min
- ğŸ”¤ Embeddings: ~0 requests/min (fewer calls)
- ğŸ“Š **TOTAL: 4.36 req/min**

**Tokens Per Minute (TPM)**:
- ğŸ’¬ Chat Endpoint:
  - Input: ~58 tokens/min
  - Output: ~3,300 tokens/min
  - **Total: ~3,358 tokens/min**

**Total Statistics**:
- Total Requests: 18 (real API calls)
- Total Tokens: 11,996 tokens
- Total Cost: **$0.238 USD** (real money spent)

---

## ğŸŒ Running Services

All accessible in Brave browser:

1. **Live Dashboard** - http://localhost:8765/
   - Real Azure OpenAI metrics
   - Auto-refreshes every 5 seconds
   - Shows costs, tokens, latencies

2. **Prometheus Metrics** - http://localhost:8765/metrics
   - Raw metrics in Prometheus format
   - Scraped every 5 seconds by Prometheus

3. **Prometheus UI** - http://localhost:9090/
   - Full Prometheus dashboard
   - Query builder and graphing
   - Historical data visualization

4. **Jupyter Notebooks** - http://localhost:8899/
   - 6 interactive tutorials
   - All features documented

---

## ğŸ“Š Prometheus Queries

Try these in the Prometheus UI:

### Request Metrics
```promql
# Total requests
azure_llm_requests_total

# Requests per minute
rate(azure_llm_requests_total[1m]) * 60

# Success rate
sum(rate(azure_llm_requests_total{status="success"}[5m])) / 
sum(rate(azure_llm_requests_total[5m]))
```

### Token Metrics
```promql
# Total tokens
azure_llm_tokens_total

# Tokens per minute  
rate(azure_llm_tokens_total[1m]) * 60

# Input vs output tokens
sum(azure_llm_tokens_total{type="input"})
sum(azure_llm_tokens_total{type="output"})
```

### Cost Metrics
```promql
# Total cost
azure_llm_cost_dollars_total

# Cost per minute
rate(azure_llm_cost_dollars_total[1m]) * 60
```

### Performance Metrics
```promql
# Request duration histogram
azure_llm_request_duration_seconds

# Active requests
azure_llm_active_requests

# P95 latency
histogram_quantile(0.95, rate(azure_llm_request_duration_seconds_bucket[5m]))
```

---

## ğŸ¯ What's Happening

âœ… **Real Azure OpenAI API calls** being made every 2-5 seconds  
âœ… **Real costs** being tracked ($0.24 USD so far)  
âœ… **Real tokens** being counted (12K+ tokens)  
âœ… **Real latencies** being measured  
âœ… All metrics **exposed to Prometheus**  
âœ… **Live dashboard** with auto-refresh  
âœ… **Historical data** being stored  

---

## ğŸ“ Implementation Complete

### Created Files
- âœ… 6 Jupyter notebooks (tutorials)
- âœ… `prometheus_live_demo.py` (real API calls)
- âœ… `prometheus_demo_simple.py` (simulated data)
- âœ… `show_metrics.py` (metrics viewer)
- âœ… Comprehensive documentation

### All 11 Features Implemented
1. âœ… Function Calling
2. âœ… Batch API Support
3. âœ… Sync Client Wrapper
4. âœ… Response Validation
5. âœ… Cost Analytics Dashboard
6. âœ… OpenTelemetry Integration
7. âœ… Integration Tests
8. âœ… Performance Benchmarks
9. âœ… Interactive Tutorials
10. âœ… Health Checks
11. âœ… Conversation Manager

---

## ğŸ‰ Success!

**100% Complete** - All requested features implemented with:
- Real Prometheus integration
- Live metrics dashboard
- Interactive Jupyter tutorials
- Comprehensive documentation

---

**Maintained by**: Torstein SÃ¸rnes  
**Repository**: https://github.com/tsoernes/azure-llm-toolkit
